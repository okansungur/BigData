### Büyük Veri
#### Giriş
Büyük hacimli verilerdir ve geleneksel veri işleme yazılımlarıyla yönetilemezler. Bu verilerin analiz edilip sınıflandırılması ve anlamlandırılması gerekmektedir. Büyük hacimli veriler; bir durum hakkında ne kadar çok şey bilindiğiyle ilgili olarak, gelecekte ne olacağı konusunda tahminlerde bulunulmasını sağlayan bir araçtır. Yakın zaman kadar veriler elektronik tablolar veya veritabanı ile sınırlıylen teknolojideki gelişimlere bağlı olarak, artık veri denilen kavram çok karmaşık bir hal almıştır. Fotoğraflardan videolara, ses kayıtlarından, yazılı metinlere ve hatta sensörlerden elde edilen verilere kadar bir çok farklı kaynağı içerisinde barındırmaktadır. Dolayısıyla kurumlar bu karmaşık yapıyı analiz ederek, müşteri profillerine göre bileşenlerine ayırarak, gelecek stratejilerini bu sonuçlara bakarak yön vermek durumundadır. Ancak bu şekilde teknolojiyi takip ettikleri sürece ayakta kalabileceklerdir. Elde edilen verilerin birbirleriyle olan ilişkislerin ortaya çıkarılması temel prensiptir. Bu ilişkileri ortaya çıkarabilmek daha sağlıklı karar alabilmenin önünü açmaktadır. Araştırmalara göre büyük veriyi kullanan şirketlerin daha fazla kazanç elde ettiği, pazar çalışmalarında göre daha etkili olduğu ve reklam harcamalarını daha doğru yaptığı ve sosyal medya kullanımında da daha başarılı oldukları gözlenmektedir. 2012 yılında İsviçre'nin Davos kentinde düzenlenen Dünya Ekonomik Forumu'nda büyük veri en önemli konulardan biriydi. Forumun "Büyük Veri, Büyük Etki" başlıklı bir raporu, verileri para birimi veya altın gibi yeni bir ekonomik varlık sınıfı olarak tanımlanmıştır [1]. Büyük Verinin öngörü gücü, halk sağlığı, ekonomik kalkınma ve ekonomik tahmin gibi alanlarda incelenmektedir. Araştırmacılar, bir bölgedeki hastanelerin acil servislerine gelen grip hastalarının sayısında bir artış olmadan birkaç hafta önce "grip semptomları" ve "grip tedavileri" gibi terimler için o bölgedeki google arama isteklerinde bir artış tespit ettiler. Dolayısıyla hastane acil servislerinde, resmi kayıtlarından önce, yaşanabilecek yoğunluk kısa süre içerisinde tahmin edilebilmiştir.

<p align="center"><b> Büyük Veri </b></p>
Büyük veri her gün karşı karşıya olduğumuz koca bir okyanus olarak düşünebilir. Bu okyanusun en büyük tedarikçileri de bizleriz. Bilgisayarlarımızdan, mobil cihazlarımızdan ve makine sensörlerinden akan bu veri kurumlar tarafından; kararları yönlendirmek, süreç ve politikaları iyileştirmek, müşteri odaklı ürün ve hizmetler oluşturmak için kullanılabilmektedir. Büyük veri sadece hacmi ile değil çeşitliliği ve karmaşıklığı nedeniyle de böyle bir tanımı hak etmektedir. Physics.org tarafından yapılan ilginç bir araştırmaya göre mevcut toplam veriyi çevrimiçi olarak indirmek için yaklaşık 181 milyon yıla ihtiyaç duyulduğu belirtilmektedir. 
<p align="center">
  <img  src="https://github.com/okansungur/BigData/blob/main/Article/r1.png">
</p>
<p align="center">
  2010'dan 2024'e kadar dünya çapında veri / bilgi hacmi (zettabyte)
</p>
Gün geçtikçe daha da önem kazanan büyük veri kavramında; kişisel mahremiyet konusu da günümüzde en çok sorgulanan kavramların başında gelmektedir.
Büyük veri geleneksel olarak kullandığımız veritabanlarındaki depolama işlemleri ve hesaplama kapasitesini aşan verilerle ifade edilir. Bu verileri kullanmaka için daha fazla işlemci kapasitesine ve farklı sistemlere ihtiyaç duyulur. Birçok büyük firma Google Amazon, Facebook yakın zamanda büyük verilerden faydalanmaya başlamıştır. Büyük verilerde 5V kavramı mevcuttur. Hacim (volume), çeşitlilik (variety), hız (velocity), doğrulama (verification,) değer (value).
**Hacim**: Terrabayttan petabaytlara kadar olan veri aralıklarını tanımlar.
**Çeşitlilik**: Veri çeşitliliği vardır ve farklı kaynaklar kullanılır.
**Hız**: Elde edilen verilerin hızlı bir şekilde elde edilip kısa zamanda veri üretilmesi gerekmektedir.
**Doğrulama**: Oldukça hızlı büyüyen bu verilerin güvenli olup olmadığının kontrolü için doğrulamaya ihtiyaç duyulmaktadır. 
**Değer**: Büyük verilerin gerekli filtrelerden geçitildikten sonra bizim için değer sağlaması önemlidir. Aslında asıl amaç veriden değer sağlanmasıdır.
Akıllı telefonların günlük yaşantımıza girmesiyle birlikte sosyal medya hesaplarında okuduğumuz, izlediğimiz, yazdığımız her işlem veri üretmektedir. Konumumuz, sağlık durumumuz, alışkanlıklarımızla ilgili aslında kendimizle ilgili bir çok veri paylaşımında bulunuruz. Bunun etik boyutunu bir kenara bırakacak olursak sağladığımız bu veriler; yapay zekanın gelişimine katkıda bulunmaktadır. Geleneksel yöntemlerle kullanılan ve yazılımlarla yönetilemeyen büyük veriler için, yeni teknolojiler geliştirilmiş ve geliştirilmeye de devam etmektedir. Tabiki bunları geliştiren kurumların başında Facebook, Amazon ve Google, Microsoft çok yüksek sayıda kullanıcıdan gelen çok büyük veriyle uğraştıkları için bu yeni yazılımları geliştirme ihtiyacı hissetmişlerdir.
Büyük veriler yapılandırılmış ve yapılandırılmamış verilerden meydana gelirler. Yapılandırılmış veriler ürün, müşteri, fiyat, adet gibi ayrılmış kategorilerden meydana gelmektedir. Yapılandırılmamış veri ise sosyal medya paylaşımları, beğeniler, tıklamalar gibi kullanıcı hareketlerini içermektedir.
##### Veri Ön İşleme
Veri Ön İşleme süreci; veri madenciliği modelleri kurulmadan önce, veri seti üzerinde yapılan düzeltmeler olarak nitelendirilir [3]. Bu işlemler tekrarlı verinin kaldırılması, eksik verilerin tamamlanması veya duruma göre çıkarılması, veri dönüştürülmesi, veri temizlenmesi, bütünleştirme gibi işlemleri içeren süreçlerdir. Amaç verilerin daha anlamlı hale getirilmesidir. En kritik süreçlerden bir tanesi, bu veri ön işleme sürecidir. Bu süreç veri madenciliğinde en fazla vakit alan süreçtir ve burada yapılacak bir hata eldeki veriyi değersizleştirebilir.
<p align="center">
  <img  src="https://github.com/okansungur/BigData/blob/main/Article/r2.png">
</p>
<p align="center">
 Veri Ön İşleme Süreçleri 
</p>

##### Hadoop
Dağıtık dosya sistemi kavramı, sıradan sunucuların disklerini bir araya getirerek büyük ve sanal bir disk oluşturan dosya sistemidir. Bu sayede çok büyük boyuttaki dosyalar sistemde saklanabilir ve üzerinde işlemler yapılabilir. Hadoop, ilişkisel veritabanı sistemlerinden farklı olarak, verileri farklı bilgisayarlarda tutan dağıtık dosya sistemi ve eşle indirge yöntemini kullanan Java programlama dili ile geliştirilmiş, açık kaynak kodlu bir kütüphanedir. Eşleme işlemi analiz edilen veri bloğu içerisinden almak istediğimiz verileri alır. İndirgeme işlemi ise alınan bu veri üzerinde yapmak istediğimiz işlemi gerçekleştirir.

<p align="center">
  <img  src="https://github.com/okansungur/BigData/blob/main/Article/r3.png">
</p>
<p align="center">
 Hadoop Ekosistemi
</p>

Hadoop, tahmini analitik, veri madenciliği ve makine öğrenimi uygulamaları da dahil olmak üzere, gelişmiş analitik işlemleri desteklemek için kullanılan, büyüyen büyük veri teknolojileri ekosisteminin merkezinde yer almaktadır. Hadoop, çeşitli yapılandırılmış ve yapılandırılmamış veri biçimlerini kullanabilir ve kullanıcılara ilişkisel veritabanlarının veya veri ambarlarının sağladığından daha fazla veri toplama, işleme ve analiz etme esnekliği sunar.
Hadoop'un ticari dağıtımları şu anda dört ana büyük veri platformu satıcısı tarafından sunulmaktadır: Amazon servisleri (AWS), Cloudera, Hortonworks ve MapR Technologies. Ek olarak, Google, Microsoft ve diğer firmalar, Hadoop ve ilgili teknolojiler üzerine kurulu bulut tabanlı yönetilen hizmetler sunar.
Büyük veri muazzam miktarda veriden oluşur. Hadoop ise bu verileri depolamak, işlemek ve analiz etmek için kullanılan kütüphanelerin topluluğudur. Dolayısıyla bu iki kavram birbirinden farklı olmasına rağmen, birbirleriyle içi içe geçmiştir

Büyük verinin günümüzde en yaygın kullanım alanı olarak müşteri ihtiyaç davranış ve tercihlerini yönelik analizler yapılmasını sağlamasıdır. Şirketlerin potansiyel hedef kitlesine rahatlıkla erişmelerini sağlar. Yakın zamanda Facebook'un kullanıcı bilgilerini Cambridge Analytica'ya satması ve Cambridge Analytica'nın bu bilgileri ABD başkanlık seçimlerinde belirli bir kitleye reklam kampanyaları gönderip, toplumun hassasiyetlerini kullanarak,
seçim sonuçlarını etkilemedeki başarısı ve buna bağlı olarak Mark Zuckerberg’in ABD Kongresi karşısına çıkıp ifade vermesi tamamen büyük veri ile ilgili bir konudur.
Büyük Veri her endüstri alanıyla doğrudan iç içedir. Bankacılık ve finans alanında, birçok farklı kanaldan gelen veriler bankacılık sektörün bu verileri yönetmek için farklı yöntemler bulmaya yöneltmiştir. Gerek müşteri memnuniyetinin sağlanmasında gerekse risk ve sahtekarlık işlemlerine karşı gereken önlemlerin alınmasında, büyük veri önemli faydalar sağlamaktadır. Diğer finans kurumlarına göre de daha avantajlı olmalarını sağlamaktadır.
Tıp alanında büyük verinin kullanılması çok büyük bir gelişmedir. Sayılı hastadan tıbbi numune almak yerine, günümüzde kullanılan akıllı saat ve giyilebilir teknolojiler sayesinde milyonlarca insandan sağlık verisi anlık olarak alınabilmektedir. Bu büyüklükteki bir veriyi analiz ederek sorunların daha hızlı ve kolayca tespit edilmesi, daha doğru teşhisler konulması ve daha doğru tedavilerin uygulanmasını mümkün olmaktadır. Eğitim alanında ise öğrencilerin hangi konularda zayıf olduklarının belirlenmesinde, hangi konuları rahatça öğrenebildiklerinin belirlenmesinde ve buna bağlı olarak, uygulanan müfredatın nasıl geliştirilmesi konusunda karar verilmesini de sağlamaktadır.









